1. Use sqoop to move data between RDBMS and HDFS

1.1 Use Sqoop to import data from MySQL DB to Hadoop cluster

  Log in Hadoop 
    cd /usr/local
  
  Download and untar the MySQL driver 
    Goto http://dev.mysql.com/downloads/file.php?id=451546 (note the link could change)
    curl -L 'dev.mysql.com/get/Downloads/Connector-J/mySQL-connector-java-5.1.30.tar.gz' | tar xz
    ls
    You should now see mysql-connector-java-5.1.30 folder
  
  Copy mySQL connector to the Sqoop lib folder
    cd mysql-connector-java-5.1.30 
    cp mysql-connector-java-5.1.30-bin.jar /usr/lib/sqoop/lib
  
  reboot
  There could be other ways of doing it but here we just use reboot for easier illustration.
  
  Now, import data from mySQL into Hive:
  
  To import all tables from mySQL into Hive (1 table):
  sqoop import --connect jdbc:mysql://mysql-server-ip:port number/mysql-db-name --username <mysql db user> --password <password> --table <mysql-table name> --hive-import --
 
  To import all tables from mySQL into Hive (all tables):
  sqoop import-all-tables --connect jdbc:mysql://mysql-server-ip:port number/mysql-db-name --username <mysql db user> --password <password> --table <mysql-table name> --hive-import --
  
  Make sure there is no opened hive shell at the same time you are running above sqoop command.

  Log in the Hadoop GUI to make sure the table is imported or use the following command:
  hadoop fs -ls /apps/hive/warehouse
  You should be able to see the imported table above
